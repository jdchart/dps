{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b66129b",
   "metadata": {},
   "source": [
    "# 2. Calculate normalized stats\n",
    "In this notebook we:\n",
    "- Load the DPS curves that were collected in step 1.\n",
    "- Calaculate stats over regular windows. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fbb4d4c",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b71411",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installs\n",
    "import sys\n",
    "!echo \"Purging pip environment and installing packages...\"\n",
    "!{sys.executable} -m pip cache purge \n",
    "!{sys.executable} -m pip uninstall -y jhutils \n",
    "!{sys.executable} -m pip install -q git+https://github.com/jdchart/jh-py-utils.git\n",
    "\n",
    "# Imports\n",
    "print(\"Importing packages...\")\n",
    "import os\n",
    "from jhutils.local_files import collect_files\n",
    "import dps\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import utils\n",
    "print(\"Ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ffad270",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "145021c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "VOSK_ANALYSES = \"/Users/jacob/Documents/Repos/dps/projects/data/Fusion/input\"\n",
    "DPS_CURVES = \"/Users/jacob/Documents/Repos/dps/projects/data/output/dps_curves/dps_curve_32_1920_20\"\n",
    "\n",
    "analysis_files = collect_files(VOSK_ANALYSES, [\"json\"])\n",
    "print(f\"Succesfully found {len(analysis_files)} files!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90e3b593",
   "metadata": {},
   "source": [
    "## Analysis config\n",
    "- `WINDOWS`: Each curve will be proportionally modifed such that the number of values becomes this resolution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75be4d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "WINDOWS = 150\n",
    "FPS = 32\n",
    "OUTPUT_DEST = \"/Users/jacob/Documents/Repos/dps/projects/data/output/norm-stats/dps_curve_32_1920_20\"\n",
    "BINS = [0, 1, 2, 3, 5]\n",
    "BIN_LABELS = [\"[0‚Äì1[\", \"[1‚Äì2[\", \"[2‚Äì3[\", \"[3‚Äì5]\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a492070d",
   "metadata": {},
   "source": [
    "## Test on one occurance\n",
    "`segment_nonzero = segment[segment > 0]` : We do this in order to evauate moment of silence. This could be optimizing by setting a minimum succesive 0 time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99732df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "INDEX = 0\n",
    "\n",
    "window_stats = []\n",
    "\n",
    "data_file_name = os.path.splitext(os.path.basename(analysis_files[INDEX]))[0]\n",
    "curve = np.load(os.path.join(DPS_CURVES, \"data\", f\"{data_file_name}.npy\"))\n",
    "speech_recognition = dps.SpeechAnalysis(analysis_files[INDEX], fps = FPS)\n",
    "\n",
    "# Convert DPS data to float64\n",
    "data = np.array(curve, dtype=np.float64)\n",
    "# data = np.nan_to_num(data, nan=0.0)\n",
    "\n",
    "total_len = len(data)\n",
    "points_per_window = total_len // WINDOWS\n",
    "\n",
    "for w in range(WINDOWS):\n",
    "    start = w * points_per_window\n",
    "    end = (w + 1) * points_per_window if w < WINDOWS - 1 else total_len\n",
    "    segment = data[start:end]\n",
    "    segment_nonzero = segment[segment > 0]\n",
    "\n",
    "    start_time = start / FPS\n",
    "    end_time = end / FPS\n",
    "\n",
    "    # Statistiques de base\n",
    "    stats = {\n",
    "        \"file\": os.path.basename(analysis_files[INDEX]),\n",
    "        \"window\": w,\n",
    "        \"start_time_s\": round(start_time, 2),\n",
    "        \"end_time_s\": round(end_time, 2),\n",
    "        \"start_time_mn\": f\"{int(start_time // 60):02}:{int(start_time % 60):02}\",\n",
    "        \"end_time_mn\": f\"{int(end_time // 60):02}:{int(end_time % 60):02}\",\n",
    "        \"count_words\": speech_recognition._count_words_region_ms(start_time * 1000, end_time * 1000),\n",
    "        \"mean\": np.mean(segment_nonzero) if len(segment_nonzero) > 0 else 0,\n",
    "        \"std\": np.std(segment_nonzero) if len(segment_nonzero) > 0 else 0,\n",
    "        \"min\": np.min(segment_nonzero) if len(segment_nonzero) > 0 else 0,\n",
    "        \"max\": np.max(segment_nonzero) if len(segment_nonzero) > 0 else 0\n",
    "    }\n",
    "\n",
    "    # R√©partition par bins\n",
    "    bin_counts, _ = np.histogram(segment_nonzero, bins=BINS)\n",
    "    for label, count in zip(BIN_LABELS, bin_counts):\n",
    "        stats[f\"count {label}\"] = count\n",
    "        stats[f\"perc {label}\"] = (count / len(segment_nonzero) * 100) if len(segment_nonzero) > 0 else 0\n",
    "\n",
    "    window_stats.append(stats)\n",
    "\n",
    "df_windows = pd.DataFrame(window_stats).round(2)\n",
    "display(df_windows.head(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba0c72c2",
   "metadata": {},
   "source": [
    "## Process all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06db26f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, source_file in enumerate(analysis_files):\n",
    "    print(f\"Treating file {i + 1}/{len(analysis_files)}...\")\n",
    "\n",
    "    window_stats = []\n",
    "\n",
    "    data_file_name = os.path.splitext(os.path.basename(source_file))[0]\n",
    "    curve = np.load(os.path.join(DPS_CURVES, \"data\", f\"{data_file_name}.npy\"))\n",
    "    speech_recognition = dps.SpeechAnalysis(source_file, fps = FPS)\n",
    "\n",
    "    # Convert DPS data to float64\n",
    "    data = np.array(curve, dtype=np.float64)\n",
    "    # data = np.nan_to_num(data, nan=0.0)\n",
    "\n",
    "    total_len = len(data)\n",
    "    points_per_window = total_len // WINDOWS\n",
    "\n",
    "    for w in range(WINDOWS):\n",
    "        start = w * points_per_window\n",
    "        end = (w + 1) * points_per_window if w < WINDOWS - 1 else total_len\n",
    "        segment = data[start:end]\n",
    "        segment_nonzero = segment[segment > 0]\n",
    "\n",
    "        start_time = start / FPS\n",
    "        end_time = end / FPS\n",
    "\n",
    "        # Statistiques de base\n",
    "        stats = {\n",
    "            \"file\": os.path.basename(source_file),\n",
    "            \"window\": w,\n",
    "            \"start_time_s\": round(start_time, 2),\n",
    "            \"end_time_s\": round(end_time, 2),\n",
    "            \"start_time_mn\": f\"{int(start_time // 60):02}:{int(start_time % 60):02}\",\n",
    "            \"end_time_mn\": f\"{int(end_time // 60):02}:{int(end_time % 60):02}\",\n",
    "            \"count_words\": speech_recognition._count_words_region_ms(start_time * 1000, end_time * 1000),\n",
    "            \"mean\": np.mean(segment_nonzero) if len(segment_nonzero) > 0 else 0,\n",
    "            \"std\": np.std(segment_nonzero) if len(segment_nonzero) > 0 else 0,\n",
    "            \"min\": np.min(segment_nonzero) if len(segment_nonzero) > 0 else 0,\n",
    "            \"max\": np.max(segment_nonzero) if len(segment_nonzero) > 0 else 0\n",
    "        }\n",
    "\n",
    "        # R√©partition par bins\n",
    "        bin_counts, _ = np.histogram(segment_nonzero, bins=BINS)\n",
    "        for label, count in zip(BIN_LABELS, bin_counts):\n",
    "            stats[f\"count {label}\"] = count\n",
    "            stats[f\"perc {label}\"] = (count / len(segment_nonzero) * 100) if len(segment_nonzero) > 0 else 0\n",
    "\n",
    "        window_stats.append(stats)\n",
    "    \n",
    "    os.makedirs(OUTPUT_DEST, exist_ok = True)\n",
    "    df_windows = pd.DataFrame(window_stats).round(2)\n",
    "    df_windows.to_csv(os.path.join(OUTPUT_DEST, f'{data_file_name}.csv'), index = True)\n",
    "\n",
    "print(\"üëç Finished!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c166a3a",
   "metadata": {},
   "source": [
    "# Process manual data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb8cfd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "MANUAL_SOURCE = \"/Users/jacob/Documents/Repos/dps/projects/data/output/dps_curves/dps_curve_LMI_Stopwatch_20230509/data/LMI_Stopwatch_20230509.npy\"\n",
    "OUTPUT_DEST = \"/Users/jacob/Documents/Repos/dps/projects/data/output/norm-stats/dps_curve_manual\"\n",
    "\n",
    "FPS = 1\n",
    "\n",
    "curve = np.load(MANUAL_SOURCE)\n",
    "window_stats = []\n",
    "\n",
    "# Convert DPS data to float64\n",
    "data = np.array(curve, dtype=np.float64)\n",
    "# data = np.nan_to_num(data, nan=0.0)\n",
    "\n",
    "total_len = len(data)\n",
    "points_per_window = total_len // WINDOWS\n",
    "\n",
    "for w in range(WINDOWS):\n",
    "    start = w * points_per_window\n",
    "    end = (w + 1) * points_per_window if w < WINDOWS - 1 else total_len\n",
    "    segment = data[start:end]\n",
    "    segment_nonzero = segment[segment > 0]\n",
    "\n",
    "    start_time = start / FPS\n",
    "    end_time = end / FPS\n",
    "\n",
    "    # Statistiques de base\n",
    "    stats = {\n",
    "        \"file\": os.path.basename(MANUAL_SOURCE),\n",
    "        \"window\": w,\n",
    "        \"start_time_s\": round(start_time, 2),\n",
    "        \"end_time_s\": round(end_time, 2),\n",
    "        \"start_time_mn\": f\"{int(start_time // 60):02}:{int(start_time % 60):02}\",\n",
    "        \"end_time_mn\": f\"{int(end_time // 60):02}:{int(end_time % 60):02}\",\n",
    "        \"mean\": np.mean(segment_nonzero) if len(segment_nonzero) > 0 else 0,\n",
    "        \"std\": np.std(segment_nonzero) if len(segment_nonzero) > 0 else 0,\n",
    "        \"min\": np.min(segment_nonzero) if len(segment_nonzero) > 0 else 0,\n",
    "        \"max\": np.max(segment_nonzero) if len(segment_nonzero) > 0 else 0\n",
    "    }\n",
    "\n",
    "    # R√©partition par bins\n",
    "    bin_counts, _ = np.histogram(segment_nonzero, bins=BINS)\n",
    "    for label, count in zip(BIN_LABELS, bin_counts):\n",
    "        stats[f\"count {label}\"] = count\n",
    "        stats[f\"perc {label}\"] = (count / len(segment_nonzero) * 100) if len(segment_nonzero) > 0 else 0\n",
    "\n",
    "    window_stats.append(stats)\n",
    "\n",
    "df_windows = pd.DataFrame(window_stats).round(2)\n",
    "display(df_windows.head(20))\n",
    "\n",
    "os.makedirs(OUTPUT_DEST, exist_ok = True)\n",
    "df_windows = pd.DataFrame(window_stats).round(2)\n",
    "df_windows.to_csv(os.path.join(OUTPUT_DEST, f'{data_file_name}.csv'), index = True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
